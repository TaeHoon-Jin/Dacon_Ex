{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_초심자 가이드 - 데이터셋부터 모델 학습까지.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaeHoon-Jin/Dacon_Ex/blob/main/PyTorch_%EC%B4%88%EC%8B%AC%EC%9E%90_%EA%B0%80%EC%9D%B4%EB%93%9C_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B%EB%B6%80%ED%84%B0_%EB%AA%A8%EB%8D%B8_%ED%95%99%EC%8A%B5%EA%B9%8C%EC%A7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrxxT83tG6vL"
      },
      "source": [
        "# **Abstract**\n",
        "\n",
        "해당 베이스라인은 저처럼 인공지능 대회에 처음 접하시는 분들이나 pytorch에 처음 접하시는 분들과 지식을 공유하고자 하는 목적으로 준비하였습니다. \n",
        "<br>\n",
        "저 또한 책과 인터넷을 찾아가며 만든 지식이라 부족할 수 있고,\n",
        "<br> \n",
        "코드의 가독성에 주의를 기울였으나 직관적이지 않은 부분, 잘 모르시겠거나 잘못된 부분은 지적해주시면 감사하겠습니다. \n",
        "<br>\n",
        "베이스라인에 모델의 구체적인 내용은 포함하지 않고 있으며 단순히 Conv과 upsampling을 토대로 한 전체적인 흐름에 대해 이해에 도움드리고자 드리는 코드입니다.\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-gPxBn-IO1e"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tarK0pZ2tAnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb530951-57c7-469e-db61-37872b5f0f55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSbEs-o0zrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6614fcc7-5ee1-4440-9508-db2bfe1289fa"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "from torchsummary import summary\n",
        "\n",
        "device = None\n",
        "\n",
        "if torch.cuda.is_available() :\n",
        "    device = torch.device('cuda')\n",
        "else : \n",
        "    device = torch.device('cpu')\n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using PyTorch version: 1.8.0+cu101  Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0F9b_NXpinN"
      },
      "source": [
        "# 데이터 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrBZi3oxpYtg"
      },
      "source": [
        "def sorted_list(path):\n",
        "    \n",
        "    tmplist = glob.glob(path)\n",
        "    tmplist.sort()\n",
        "    \n",
        "    return tmplist\n",
        "\n",
        "def read_csv(path):\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def load_npy(path):\n",
        "    \n",
        "    npy = np.load(path)\n",
        "    \n",
        "    return npy\n",
        "\n",
        "def show_data(npy):\n",
        "    \n",
        "    num_channel = npy.shape[-1]\n",
        "    plt.figure(figsize=(3*num_channel, 5))\n",
        "    for channel in range(num_channel):\n",
        "        tmpimg = npy[:, :, channel]\n",
        "        plt.subplot(1, num_channel, channel+1)\n",
        "        plt.imshow(tmpimg)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oQ_rhH7pZeF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWmu4hjwpZk7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbGvbwJKIwEk"
      },
      "source": [
        "# **PyTorch Computer Vision cookbook** - Michael Avendi\n",
        "이 책의 내용이 다수 포함되어 있으며, 파이토치를 처음 접하신다면 적극추천하는 책입니다.\n",
        "영어로 되어있지만 코드가 매우 이해하기 쉽게 설명되어 있습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QmiuW3_JY0j"
      },
      "source": [
        "# **Creating Dataset**\n",
        "\n",
        "저의 경우, 482장의 데이터파일을 8개씩 묶어 6장을 통해 뒤의 2장을 예측할 수 있도록 분리하여 저장하도록 하였습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E04ldC0Lj21f",
        "outputId": "9bbb4bcc-281f-47d8-fcda-ccfb32775c8d"
      },
      "source": [
        "data_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./drive/MyDrive/Colab Notebooks/Dacon/Monthly Dacon/Monthly Dacon 13/data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo8RGa-Na7Z1"
      },
      "source": [
        "submission = pd.read_csv('./drive/MyDrive/Colab Notebooks/Dacon/Monthly Dacon/Monthly Dacon 13/data/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Zx8uUSko-v",
        "outputId": "4573b444-6a99-455f-d8ba-8b06aceccb82"
      },
      "source": [
        "submission.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 136193)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSxIUrB01Has",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c7cab5-8048-423e-ed87-58bdbf1d0333"
      },
      "source": [
        "data_path = './drive/MyDrive/Colab Notebooks/Dacon/Monthly Dacon/Monthly Dacon 13/data'\n",
        "train_data_path = os.path.join(data_path, \"train\")\n",
        "print(train_data_path)\n",
        "file_list = os.listdir(train_data_path)\n",
        "file_list.sort()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./drive/MyDrive/Colab Notebooks/Dacon/Monthly Dacon/Monthly Dacon 13/data/train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WlYMIZvkfxT0",
        "outputId": "f6c30e0f-cfb9-4772-feab-5bbf18bc90cc"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU7GpxiZCNHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cc8f4d-c679-44d2-e551-8134b8df042d"
      },
      "source": [
        "## Custom Dataset which return x_frames, y_frames\n",
        "\n",
        "torch.set_printoptions(threshold=10000) # show all tensor without abbreviation\n",
        "\n",
        "class SeaIceDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform, data_type=\"train\", frame_num=6, predict_num=2, stride=1):\n",
        "        super(SeaIceDataset, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        data_dir                => data folder path\n",
        "        transform               => data to tensor\n",
        "        data_type=\"train\"       => choose train / valid / test\n",
        "        frame_num               => frame nums to use on train \n",
        "        predict_num             => frame nums to predict\n",
        "        stride_num              => stride for frames (if stride=2 => 197811.npy, 198001.npy, 198003.npy ... )\n",
        "                                   만약 8월끼리 비교하고 싶다면 stride = 12 를 넣어준다.\n",
        "        \"\"\"\n",
        "\n",
        "        data_to_path = os.path.join(data_dir, data_type)\n",
        "        filenames = os.listdir(data_to_path)\n",
        "        self.filepaths = [os.path.join(data_to_path, filename) for filename in sorted(filenames)]\n",
        "        \n",
        "        self.transform = transform  # numpy 배열을 tensor 배열로 바꿔주는 함수\n",
        "        self.frame_num = frame_num \n",
        "        self.predict_num = predict_num\n",
        "        self.stride = stride\n",
        "\n",
        "    def __len__(self):\n",
        "        # len = dataset으로 시작가능한 인덱스 번호 \n",
        "        return len(self.filepaths) - (self.frame_num + self.predict_num - 1) * self.stride\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        it will return (x_with_frame_num, y_true_with_predict_num)\n",
        "        if frame_num = 6, predict_num = 2\n",
        "        ((6, 1, 448, 304), (2, 1, 448, 304))\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for id in range(idx, idx + self.frame_num + self.predict_num, self.stride):\n",
        "            cur_npy = np.load(self.filepaths[id])[:,:,0]/250    # 250을 나눠주어 저장하지 않으면 toTensor했을때 오차값이 크게 생겼습니다\n",
        "            cur_tensor = self.transform(cur_npy)                # tensor로 저장\n",
        "            dataset.append(cur_tensor)\n",
        "        x = torch.stack(dataset[:self.frame_num])\n",
        "        x = x.transpose(0,1).to(dtype=torch.float)              # [1, 6, 448, 304] => [channel, frames, height, width]\n",
        "        y = torch.stack(dataset[self.frame_num:])               \n",
        "        y = y.transpose(0,1)                                    # [1, 2, 448, 304] => [channel, frames, height, width]\n",
        "        return x, y\n",
        "\n",
        "def getTransform():\n",
        "    return transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "\n",
        "transform = getTransform()\n",
        "\n",
        "ice_dataset = SeaIceDataset(data_path, transform, \"train\", 6, 2, 1)\n",
        "\n",
        "a,b = ice_dataset[1]        # sample to see \n",
        "print(len(ice_dataset))     # 데이터셋에 있는 총 데이터의 개수는 8개씩 묶여있는 475개의 데이터가 있습니다\n",
        "print(a.shape, b.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "475\n",
            "torch.Size([1, 6, 448, 304]) torch.Size([1, 2, 448, 304])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkO8KEdlDODU",
        "outputId": "4c88095e-01f0-46d0-c43e-e39cb3b19900"
      },
      "source": [
        "# Create splited Dataset to Train and Valid\n",
        "\n",
        "len_ice_dataset = len(ice_dataset)\n",
        "len_ice_train = int(0.8*len_ice_dataset)\n",
        "len_ice_valid = len_ice_dataset - len_ice_train\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset = random_split(ice_dataset, [len_ice_train, len_ice_valid])\n",
        "print(f\"train dataset length : {len(train_dataset)}\")\n",
        "print(f\"valid dataset length : {len(valid_dataset)}\")\n",
        "\n",
        "# show one of train_ds\n",
        "for x, y in train_dataset:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train dataset length : 380\n",
            "valid dataset length : 95\n",
            "torch.Size([1, 6, 448, 304]) torch.Size([1, 2, 448, 304])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHROCWIgGXPL",
        "outputId": "6996f30e-a24c-4e8d-9356-69f95af50b76"
      },
      "source": [
        "# Creating DataLoader\n",
        "# too slow now...\n",
        "\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "# Dataloader 클래스는 데이터셋에서 배치 개수만큼 뽑아서 제공해줍니다\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# show example\n",
        "# train_dataloader length => 32\n",
        "for x, y in train_dataloader:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12, 1, 6, 448, 304]) torch.Size([12, 1, 2, 448, 304])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9OZvEkrNV8a"
      },
      "source": [
        "# **Building Model**\n",
        "\n",
        "학습시킬 모델의 구조에 대해 알아보려고 합니다\n",
        "<br>\n",
        "모델의 경우, 사용될 레이어를 init에 초기화하고 forward에서 적용하는 방식으로 진행됩니다\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1rME0m1LXIn"
      },
      "source": [
        "MODEL_PARAMS = {\n",
        "    \"shape\" : (6, 1, 448, 304),\n",
        "    \"init_filters\": 8,\n",
        "    \"dropout_rate\" : 0.5\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeQUzLESGZJt"
      },
      "source": [
        "# Creating Model\n",
        "\n",
        "class CustomNet(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(CustomNet, self).__init__()\n",
        "        input_frames, input_channel, input_height, input_width = params[\"shape\"] # input_frames? input_batch?\n",
        "        init_filters = params[\"init_filters\"]\n",
        "        self.dropout_rate = params[\"dropout_rate\"]\n",
        "        self.conv1 = nn.Conv3d(input_channel, init_filters, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv3d(init_filters, init_filters*2, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.ConvTranspose3d(init_filters*2, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool3d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.upsample(x, size=(2, 448, 304))\n",
        "        print(\"input: \", input.shape)\n",
        "        print(\"output: \", x.shape)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6ICr-zfLu3Z",
        "outputId": "fd0e93aa-6f50-4aae-a00d-f2c537e233f2"
      },
      "source": [
        "my_model = CustomNet(MODEL_PARAMS).to(device) # to(device) 해줘야 에러가 안남\n",
        "print(my_model)\n",
        "summary(my_model, input_size=(1, 6, 448, 304), device=device.type) # summary 함수를 통해 임의의 사이즈를 넣어 구조와 파라미터를 확인할 수 있습니다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CustomNet(\n",
            "  (conv1): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv2): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (conv3): ConvTranspose3d(16, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            ")\n",
            "input:  torch.Size([2, 1, 6, 448, 304])\n",
            "output:  torch.Size([2, 1, 2, 448, 304])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1       [-1, 8, 6, 448, 304]             224\n",
            "            Conv3d-2      [-1, 16, 3, 224, 152]           3,472\n",
            "   ConvTranspose3d-3       [-1, 1, 3, 224, 152]             433\n",
            "================================================================\n",
            "Total params: 4,129\n",
            "Trainable params: 4,129\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 3.12\n",
            "Forward/backward pass size (MB): 63.12\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 66.26\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ij5v3ZUOhZG"
      },
      "source": [
        "# **Loss Function & Metric Function**\n",
        "\n",
        "손실함수에 mae_over_f1을 매트릭함수에 mae_score, f1_score를 넣었습니다.\n",
        "해당 과정에서도 np는 tensor로 변경해주어야 합니다\n",
        "\n",
        "<br>\n",
        "이전의 결과값이 250\\*0.05 < y < 250\\*0.5 였지만  \n",
        "데이터셋에서 250을 나눠주는 과정을 겪었기 때문에 여기서도 250대신 1을 이용하였습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F63xqplnL7D9"
      },
      "source": [
        "# Loss Function && etric Function\n",
        "\n",
        "# metrics\n",
        "def mae_score(true, pred):\n",
        "    true, pred = numpy_to_tensor(true, pred)\n",
        "    score = np.mean(np.abs(true-pred))\n",
        "    \n",
        "    return score\n",
        "\n",
        "# metrics\n",
        "def f1_score(true, pred):\n",
        "    true, pred = numpy_to_tensor(true, pred)\n",
        "\n",
        "    target = np.where((true>1*0.05)&(true<1*0.5))\n",
        "    \n",
        "    true = true[target]\n",
        "    pred = pred[target]\n",
        "    true = np.where(true < 1*0.15, 0, 1)\n",
        "    pred = np.where(pred < 1*0.15, 0, 1)\n",
        "    \n",
        "    right = np.sum(true * pred == 1)\n",
        "    precision = right / np.sum(true+1e-8)\n",
        "    recall = right / np.sum(pred+1e-8)\n",
        "    score = 2 * precision*recall/(precision+recall+1e-8)\n",
        "    \n",
        "    return score\n",
        "    \n",
        "# loss function\n",
        "def mae_over_f1(true, pred):\n",
        "    mae = mae_score(true, pred)\n",
        "    f1 = f1_score(true, pred)\n",
        "    score = mae/(f1+1e-8)\n",
        "    \n",
        "    return score\n",
        "\n",
        "def numpy_to_tensor(true, pred):\n",
        "    return true.cpu().detach().numpy(), pred.cpu().detach().numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUfPbmksP7Xn"
      },
      "source": [
        "# **Optimizer**\n",
        "\n",
        "최적화는 아담을 이용하였습니다. \n",
        "<br>\n",
        "시간이 지남에 따라 학습률을 감소시킴으로써 정확도를 높일 수 있을 것 같습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh770cUrSgt2",
        "outputId": "a26e4860-5674-46c5-827b-68cbe5550afc"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "# adam optimizer\n",
        "opt_adam = optim.Adam(my_model.parameters(), lr=3e-4)\n",
        "\n",
        "def get_lr(opt):\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "# check our learning rate\n",
        "current_lr = get_lr(opt_adam)\n",
        "print(f\"current_lr = {current_lr}\")\n",
        "\n",
        "\n",
        "\n",
        "# learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(opt_adam, mode=\"min\", factor=0.5, patience=20, verbose=1)\n",
        "\n",
        "# example \n",
        "for i in range(100):\n",
        "    lr_scheduler.step(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current_lr = 0.0003\n",
            "Epoch    22: reducing learning rate of group 0 to 1.5000e-04.\n",
            "Epoch    43: reducing learning rate of group 0 to 7.5000e-05.\n",
            "Epoch    64: reducing learning rate of group 0 to 3.7500e-05.\n",
            "Epoch    85: reducing learning rate of group 0 to 1.8750e-05.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNgzvvwQ6E_"
      },
      "source": [
        "# **Training Setting**\n",
        "\n",
        "에폭 한번할때마다 loss_epoch함수를 실행하는데\n",
        "<br>\n",
        "dataloader의 x,y 마다 \n",
        "<br>\n",
        "metric_batch를 통해 metric값을 계산하고\n",
        "<br>\n",
        "loss_batch를 통해 loss 값을 계산합니다\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERdhQi1zUEEd"
      },
      "source": [
        "# Training \n",
        "\n",
        "def metrics_batch(pred, true, metrics):\n",
        "    # if needed add param \"metrics\" to custom\n",
        "    \"\"\"\n",
        "    output will be pred\n",
        "    target will be corrects\n",
        "    \"\"\"\n",
        "    if metrics:\n",
        "        return list(map(lambda x: x(true, pred), metrics))\n",
        "    mae_score = mae_score(true, pred)\n",
        "    f1_score = f1_score(true, pred)\n",
        "    return (mae_score, f1_score)\n",
        "\n",
        "def loss_batch(loss_func, pred, true, opt=None):\n",
        "    \"\"\"\n",
        "    loss_func => mae_over_f1\n",
        "    \"\"\"\n",
        "    loss = loss_func(true, pred)\n",
        "    with torch.no_grad():\n",
        "        metrics = metrics_batch(pred, true, [mae_score, f1_score])\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        # loss.backward()\n",
        "        opt.step()  # 학습이 이뤄지는 곳\n",
        "    return loss, metrics\n",
        "\n",
        "def loss_epoch(model, loss_func, dataset_dataloader, sanity_check=False, opt=None):\n",
        "    running_loss = 0.0\n",
        "    running_metric = [0.0, 0.0]\n",
        "    len_data = len(dataset_dataloader.dataset)\n",
        "\n",
        "    for x, y in dataset_dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # 모델 결과\n",
        "        pred = model(x)\n",
        "        # 손실함수 구하기\n",
        "        loss, metrics = loss_batch(loss_func, pred, y, opt)\n",
        "        # 손실함수 \n",
        "        running_loss += loss\n",
        "        if metrics is not None:\n",
        "            for idx, metric_value in enumerate(metrics):\n",
        "                running_metric[idx] += metric_value\n",
        "        \n",
        "        # 문제 있으면 break, 여기서는 True 일때 바로 break\n",
        "        if sanity_check is True:\n",
        "            break\n",
        "    \n",
        "    loss = running_loss / float(len_data)\n",
        "    metrics = list(map(lambda x: x/float(len_data), metrics))\n",
        "    print(loss, metrics)\n",
        "    return loss, metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dduXzCrsb3eI"
      },
      "source": [
        "loss_func = mae_over_f1\n",
        "opt_adam = optim.Adam(my_model.parameters(), lr=3e-4)\n",
        "lr_scheduler = ReduceLROnPlateau(opt_adam, mode=\"min\", factor=0.5, patience=20, verbose=1)\n",
        "\n",
        "TRAIN_PARAMS = {\n",
        "    \"num_epochs\" : 10,\n",
        "    \"loss_func\" : loss_func,\n",
        "    \"optimizer\" : opt_adam,\n",
        "    \"train_dataloader\" : train_dataloader,\n",
        "    \"valid_dataloader\" : valid_dataloader,\n",
        "    \"sanity_check\" : True,\n",
        "    \"lr_scheduler\" : lr_scheduler,\n",
        "    \"save_path\" : \"./weights.pt\"\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgCENlxJVZkx"
      },
      "source": [
        "def train(model, params):\n",
        "    num_epochs = params['num_epochs']\n",
        "    loss_func = params['loss_func']\n",
        "    opt = params[\"optimizer\"]\n",
        "    train_dataloader = params['train_dataloader']\n",
        "    valid_dataloader = params['valid_dataloader']\n",
        "    sanity_check = params['sanity_check']\n",
        "    lr_scheduler = params['lr_scheduler']\n",
        "    save_path = params['save_path']\n",
        "\n",
        "    # keep history of the loss and metric\n",
        "    loss_hist = {\n",
        "        \"train\" : [],\n",
        "        \"valid\" : []\n",
        "    }\n",
        "\n",
        "    metrics_hist = {\n",
        "        \"train\" : [],\n",
        "        \"valid\" : []\n",
        "    }\n",
        "\n",
        "    # copy best weights\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    # init best loss\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        current_lr = get_lr(opt)\n",
        "        print(f'Epoch:{epoch}/{num_epochs-1}, current lr:{current_lr}')\n",
        "        model.train()\n",
        "        train_loss, train_metrics = loss_epoch(model, loss_func, train_dataloader, sanity_check, opt)\n",
        "\n",
        "        # save history\n",
        "        loss_hist[\"train\"].append(train_loss)\n",
        "        metrics_hist[\"train\"].append(train_metrics)\n",
        "\n",
        "        # model.eval()\n",
        "        # with torch.no_grad():\n",
        "    \n",
        "\n",
        "    return model, loss_hist, metrics_hist\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa7DHkUuiWGg",
        "outputId": "7dafb04b-8ce7-47fa-c2c1-df27c546775a"
      },
      "source": [
        "my_model, loss_hist, metrics_hist = train(my_model, TRAIN_PARAMS)\n",
        "\n",
        "print(loss_hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0/9, current lr:0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173459509091 [0.00047206722502364897, 0.0013883080755960155]\n",
            "Epoch:1/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173459191517 [0.0004720672250068951, 0.0013883080755960155]\n",
            "Epoch:2/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173458064715 [0.00047206722494744983, 0.0013883080755960155]\n",
            "Epoch:3/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173457354976 [0.00047206722491000713, 0.0013883080755960155]\n",
            "Epoch:4/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173459106694 [0.0004720672250024202, 0.0013883080755960155]\n",
            "Epoch:5/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173458611969 [0.00047206722497632067, 0.0013883080755960155]\n",
            "Epoch:6/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173457983189 [0.0004720672249431489, 0.0013883080755960155]\n",
            "Epoch:7/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173458829598 [0.00047206722498780183, 0.0013883080755960155]\n",
            "Epoch:8/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173457410122 [0.00047206722491291637, 0.0013883080755960155]\n",
            "Epoch:9/9, current lr:0.0003\n",
            "input:  torch.Size([12, 1, 6, 448, 304])\n",
            "output:  torch.Size([12, 1, 2, 448, 304])\n",
            "0.0008948173457686081 [0.0004720672249274748, 0.0013883080755960155]\n",
            "{'train': [0.0008948173459509091, 0.0008948173459191517, 0.0008948173458064715, 0.0008948173457354976, 0.0008948173459106694, 0.0008948173458611969, 0.0008948173457983189, 0.0008948173458829598, 0.0008948173457410122, 0.0008948173457686081], 'valid': []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzB3jHe7zeHo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}